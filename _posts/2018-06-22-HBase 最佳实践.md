# HBase 最佳实践

标签（空格分隔）： HBase

---

**HBase介绍**
> * 高可靠、高性能、面向列、可伸缩，实时读写的分布式数据库
> * 利用HDFS作为文件存储系统
> * 利用Zookeeper作为分布式协同服务
> * 主要存储非结构化和半结构化的松散数据

---
**行式存储与列式存储**
![QQ截图20180622103755.png-43.6kB][1]
列式数据库在并行查询处理上更有优势。而且数据是以列为单元存储，要查询哪些列上的数据，直接读取列就行了。不像行式数据库，一行上有多列，尽管我们只要分析某些列的数据，也要读取表上的全部数据比如“select a,b from table limit 100”列式数据库需要读取a,b这两列的100行到内存中，而行式数据库则需要将前100行数据都存入内存中，列式数据库I/O更高效。

列存数据库是用来支持一部分数据仓库和集市的应用的。传统数据库做统计分析时最大的瓶颈就是IO数据。所以列存的主要就是99%的操作都是查询应用，数据库的更新操作是计划而不是随机的。列存的单条记录的增删改查性能一般，甚至比行数据库差不少，但汇总查询超快，适合标准的星型模型，多维查询。

传统的行式数据库，是按照行存储的，维护大量的索引和物化视图无论是在时间(处理)还是空间(存储)方面成本都很高。而列式数据库恰恰相反，列式数据库的数据是按照列存储，每一列单独存放，数据即是索引。只访问查询涉及的列，大大降低了系统I/O，每一列由一个线来处理，而且由于数据类型一致，数据特征相似，极大方便压缩。

---
 **列式数据库的优缺点**

行式数据库擅长随机读操作，列式数据库则更擅长大批量数据量查询

列式数据库优点：

极高的装载速度 (最高可以等于所有硬盘IO 的总和，基本是极限了)

适合大量的数据而不是小数据

实时加载数据仅限于增加（删除和更新需要解压缩Block 然后计算然后重新压缩储存）

高效的压缩率，不仅节省储存空间也节省计算内存和CPU.

非常适合做聚合操作.


缺点：

不适合扫描小量数据

不适合随机的更新

批量更新情况各异，有的优化的比较好的列式数据库（比如Vertica)表现比较好，有些没有针对更新的数据库表现比较差.

不适合做含有删除和更新的实时操作.

---
HBase架构

![ml2m3ni7xc.png-187.9kB][2]

---
HMaster节点
管理HRegionServer，实现其负载平衡
HRegion split时分配新的或在HRegionServer退出时，负责将HRegion迁移到其它Server
实现DDL操作（namespace和table的增删改查，CF的增删改查）
权限管理控制（ACL）

---
RegionServer

存放和管理本地的HRegion
读写HDFS，管理Table中的数据
Client直接通过HRegionServer读写数据

![图片1.jpg-45.8kB][3]

HBase使用rowkey将表切分成多个Region每个Region都记录了startkey和endkey，并且是有序的。每个RegionServer可以同时管理1000个左右的HRegion

![图片2.jpg-36kB][4]

Region虽然是最小的分布式单元，但不是最小的存储单元Region由一个或者多个Store组成，每个store保存一个column family，每个Store又由一个memStore和多个StoreFile组成，memstore存储在内存中，StoreFile存储在HDFS上。

Client写入 ---》存入MemStore，一直到MemStore存满记达到阈值----》flush成StoreFile，直至storeFile的数量增长到
一个阈值------》出发Compact合并操作---》多个storeFile合并成一个StoreFile
同时进行版本合并和数据删除-------》当StoreFile Compact后逐步形成越来越大的StoreFile
单个StoreFile超过一定预知后，会出发split操作，把当前region split成两个region，Region会下线，
新split出的两个Region会被HMaster分配到相应的HRegionServer上，使得原先一个Region的压力分流到两个Region上。

---
HBase Region的名称为HDFS文件名称
![1.jpg-87.8kB][5]

![2.jpg-102.6kB][6]

在海量的数据中查找数据
     1.依据rowkey查询最快
        2.对rowkey进行范围查询range
        3.前缀匹配
Get最快的数据查询

---
ResultScanner类
扫描操作不会通过一次RPC返回所有匹配的行，而是以行为单位进行返回。同时在一次请求中发送大量数据，会占用大量的系统资源并消耗很长时间。
ResultScanner把扫描操作转换为类似的get操作，它将每一行数据封装成一个Result实例，并将所有的Result放入到一个迭代器中。

---
**大量数据导入HBase Compact思路**

阻塞时候的思路
1.RS 内存设置太小 memstore默认占40%
2.HF达到允许的最大数量  具体看 hbase.hstore.blockingStoreFiles的设置
3.MS大小达到阈值 hbase.hregion.memstore.flush.size * hbase.hregion.memstore.block.multiplier
4.RS上面的MS总大小达到阈值 hbase.regionserver.global.memstore.size

http://hbasefly.com/2016/06/18/hbase-practise-ram/#comment-5141

合并：
存储文件会被后台的管理进程存储起来以确保他们在控制之下，随着Memstore的刷写会产生很多小文件，当文件数目达到阈值时便会进行
合并，将把它们合并成数量更少的体积更大的文件。这个过程持续到这些文件中最大的文件超过配置最大存储时的大小，此时便会触发一个Region的拆分。

压缩方式有两种：
minor和major
minor合并负责重写最后生成的几个文件到一个更大的文件中，文件数量由
hbase.hstore.compaction.min属性设置默认值为3，最小应该大于等于2 至少有>=2个storefile满足条件合并才会启动
hbase.hstore.compaction.max 10 一次合并选择 10 storefile
hbase.hstore.compaction.min.size 128M storefile小于该值的总是会被合并
hbase.hstore.compaction.max.size  storefile大于该值的总是不会被合并
hbase.hstore.compaction.ratio 默认值为1.2来确保在选择过程中有足够多的文件。保证老文件一定被合并

major合并，它们把所有文件压缩成一个单独的文件

删除那些被标记的数据，超过时间（TTL）数据，超过版本数量限制的数据，major合并会消耗更多的资源，合并进行时也会影响HBase的相应时间。在HBase 0.96之前，默认每天对region做一次major compact，现在这个周期被改成了7天。然而，因为major compact可能导致某台server短时间内无法响应客户端的请求，如果无法容忍这种情况的话，可以关闭自动major compact，改成在请求低谷期手动触发这一操作

拆分合并风暴：
当用户的Region大小以恒定的速度保持增长时，region的拆分会在同一时间发生，因为同时要压缩region中的数据文件，这个过程会拆分重写之后的region这将引起磁盘的I/O上升。与其依赖Hbase自动管理拆分不如自己手动split和major——compact
不要将hbase.hregion.max.filesize值设置为Long.MAX_VALUE用户最好为这个值设置一个合理的上限。

使用配置优化集群：
1.减少zk超时的发生
2.增加处理线程  hbase.regionServer.handler.count 单次访问请求开销较小时可以提升处理线程数量
3.增加堆大小
    HBase分配8G或者更大的内存空间
4.启用数据压缩
    用户当为存储文件启用压缩，snappy或者LZO压缩
5.增加region大小
    更大的region可以减少集群的region总数，管理较少的region可以让集群更平稳，默认region大小为256M，用户可以设置为更大，大的region
也意味着在高负载的情况下合并停顿的时间更长。
6.调整块缓存大小
   控制堆中块缓存大小的属性是一个用浮点数类型的比值默认为20%及0.2
   perf.hfile.block.cache.size属性可以改变这个百分比
   用户负载大多数为读请求是另一个增加块缓存的原因，增加块缓存大小可以帮助用户存储更多的数据。
   块缓存与memstore的上限不能超过100%，用户需要为其他操作保留空间
7.限制memstore
    内存存储占用的堆大小用hbase.regionserver.global.memstore.upperLimit属性来配置默认值为0.4
hbase.regionserver.global.memstore.lowerLimit属性设置为0.35用于控制清除memstore之后剩余的大小，将上限与下限设置的接近一些避免过度刷写
在主要处理读请求时可以时同时减少memstore上下限来增加块缓存空间。
处理许多写请求时应该检查日志文件，如果刷写的数据量都很小，如5M，用户就有必要通过增加存储的限制来降低过度的I/O操作。
8.增加阻塞时存储文件数目 hbase.hstore.blockingStoreFiles
9.增加阻塞倍率
    hbase.hregion.memstore.block.multiplier的默认值为2，它是一个用于阻塞客户端更新请求的安全阈值memstore达到multiplier*flush的大小限制时会阻止进一步的更新。
10.减少最大日志限制
     设置hbase.regionserver.maxlog，使用户能够控制基于磁盘的WAL文件数进一步控制刷写频率，默认为32.对于写压力比较大的应用来说这个值有点高。降低这个值会迫使服务器更频繁的将数据写到磁盘上

 
    


HBase中内存规划直接涉及读缓存BlockCache，写缓存MemStore，影响系统利用率
I/O利用率资源以及读写性能，重要性不言而喻。

案例：写多多少+LRUBlockCache
内存分布图：

图中分配给RegionServer进程的内存就是JVM内存，主要分为三部分：LRUBlockCache，用于读缓存；MemStore，用于写缓存；Other，用于RS运行所必须的其他对象；
了解了BucketCache模式下的内存分布图之后，我们具体来分析如何规划内存，首先列出来基本条件：
a.  整个物理机内存：96G
b.  业务负载分布：30%读，70%写
接下来将问题一步一步分解，从上至下按照逻辑对内存进行规划：
(1) 系统内存基础上如何规划RS内存？
这个问题需要根据自身服务器情况决定，一般情况下，在不影响其他服务的情况下，越大越好。我们目前设置为64G，为系统内存的2/3。
(2) 如何设置LRUBlockCache、MemStore？
确定RegionServer总内存之后，接下来分别规划LRUBlockCahce和MemStore的总内存。在此需要考虑两点：在写多读少的业务场景下，写缓存显然应该分配更多内存，读缓存相对分配更少；HBase在此处有个硬规定：LRUBlockCache + MemStore < 80% * JVM_HEAP，否则RS无法启动。
推荐内存规划：MemStore = 45% * JVM_HEAP = 64G * 45% = 28.8G ，LRUBlockCache = 30% * JVM_HEAP = 64G * 30% = 19.2G；默认情况下Memstore为40% * JVM_HEAP，而LRUBlockCache为25% * JVM_HEAP
配置设置实践
（1）设置JVM参数如下：
-XX:SurvivorRatio=2  -XX:+PrintGCDateStamps  -Xloggc:$HBASE_LOG_DIR/gc-regionserver.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=1 -XX:GCLogFileSize=512M -server -Xmx64g -Xms64g -Xmn2g -Xss256k -XX:PermSize=256m -XX:MaxPermSize=256m -XX:+UseParNewGC -XX:MaxTenuringThreshold=15  -XX:+CMSParallelRemarkEnabled -XX:+UseCMSCompactAtFullCollection -XX:+CMSClassUnloadingEnabled -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=75 -XX:-DisableExplicitGC
（2）hbase-site.xml中MemStore相关参数设置如下：
<property>
    <name>hbase.regionserver.global.memstore.upperLimit</name>
    <value>0.45</value>
</property>
<property>
    <name>hbase.regionserver.global.memstore.lowerLimit</name>
    <value>0.40</value>
</property>
由上述定义可知，hbase.regionserver.global.memstore.upperLimit设置为0.45，hbase.regionserver.global.memstore.lowerLimit设置为0.40hbase.regionserver.global.memstore.upperLimit表示RegionServer中所有MemStore占有内存在JVM内存中的比例上限。如果所占比例超过这个值，RS会首先将所有Region按照MemStore大小排序，并按照由大到小的顺序依次执行flush，直至所有MemStore内存总大小小于hbase.regionserver.global.memstore.lowerLimit，一般lowerLimit比upperLimit小5%。
（3）hbase-site.xml中LRUBlockCache相关参数设置如下：
<property>
    <name>hfile.block.cache.size</name>
    <value>0.3</value>
</property>
hfile.block.cache.size表示LRUBlockCache占用内存在JVM内存中的比例，因此设置为0.3

案例二：读多写少型+BucketCache


如图，整个RegionServer内存（Java进程内存）分为两部分：JVM内存和堆外内存。其中JVM内存中LRUBlockCache和堆外内存BucketCache一起构成了读缓存CombinedBlockCache，用于缓存读到的Block数据，其中LRUBlockCache用于缓存元数据Block，BucketCache用于缓存实际用户数据Block；MemStore用于写流程，缓存用户写入KeyValue数据；还有部分用于RegionServer正常运行所必须的内存；
内存规划思路
和案例一相同，本案例中物理机内存也是96G，不过业务类型为读多写少：70%读+30%写
因为BucketCache模式下内存分布图相对复杂，我们使用如下表格一步一步对内存规划进行解析：
序号 步骤 原理 计算公式 计算值 修正值
A 规划RS总内存  在系统内存允许且不影响其他服务的情况下，越多越好。设置为系统总内存的 2/3。 2/3 * 96G 64G 64G
B 规划读缓存 CombinedBlockCache 整个RS内存分为三部分：读缓存、写缓存、其他。基本按照5 : 3 : 2的分配原则。读缓存设置为整个RS内存的50% A * 50% 32G 34G
B1 规划读缓存LRU部分 LRU部分主要缓存数据块元数据，数据量相对较小。设置为整个读缓存的10% B * 10% 3.2G 3G
B2 规划读缓存BucketCache部分 BucketCache部分主要缓存用户数据块，数据量相对较大。设置为整个读缓存的90% B * 90% 28.8G 30G
C 规划写缓存MemStore 整个RS内存分为三部分：读缓存、写缓存、其他。基本按照5:4:1的分配原则。写缓存设置为整个RS内存的40% A * 30% 19.2G 20G
D 设置JVM_HEAP RS总内存大小 – 堆外内存大小 A – B2  35.2G 30G
计算修正
看到这里，可能很多仔细的朋友就会疑问，案例一不是说过HBase有一个硬规定么：LRUBlockCache + MemStore < 80% * JVM_HEAP，否则RS无法启动。不错，HBase确实有这样一个规定，这个规定的本质是为了在内存规划的时候能够给除过写缓存和读缓存之外的其他对象留够至少20%的内存空间。那按照上述计算方式能不能满足这个硬规定呢，LRU + MemStore / JVM_HEAP = 3.2G + 19.2G / 35.2G = 22.4G / 35.2G =  63.6% ，远小于80%。因此需要对计算值进行简单的修正，适量减少JVM_HEAP值（减少至30G），增大Memstore到20G。因为JVM_HEAP减少了，堆外内存就需要适量增大，因此将BucketCache增大到30G。

调整之后，LRU + MemStore / JVM_HEAP = 3.2G + 20G / 30G = 23.2G / 30G =  77%
 配置设置实践
（1）设置JVM参数如下：
-XX:SurvivorRatio=2  -XX:+PrintGCDateStamps  -Xloggc:$HBASE_LOG_DIR/gc-regionserver.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=1 -XX:GCLogFileSize=512M -server -Xmx40g -Xms40g -Xmn1g -Xss256k -XX:PermSize=256m -XX:MaxPermSize=256m -XX:+UseParNewGC -XX:MaxTenuringThreshold=15  -XX:+CMSParallelRemarkEnabled -XX:+UseCMSCompactAtFullCollection -XX:+CMSClassUnloadingEnabled -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=75 -XX:-DisableExplicitGC
（2）hbase-site.xml中MemStore相关参数设置如下：
<property>
    <name>hbase.regionserver.global.memstore.upperLimit</name>
    <value>0.66</value>
</property>
<property>
    <name>hbase.regionserver.global.memstore.lowerLimit</name>
    <value>0.60</value>
</property>
根据upperLimit参数的定义，结合上述内存规划数据可计算出 upperLimit =  20G / 30G = 66%。因此upperLimit参数设置为0.66，lowerLimit设置为0.60
（3）hbase-site.xml中CombinedBlockCache相关参数设置如下：
<property>
    <name>hbase.bucketcache.ioengine</name>
    <value>offheap</value>
</property>
<property>
    <name>hbase.bucketcache.size</name>
    <value>34816</value>
</property>
<property>
    <name>hbase.bucketcache.percentage.in.combinedcache</name>
    <value>0.90</value>
</property>
按照上述介绍设置之后，所有关于内存相关的配置基本就完成了。但是需要特别关注一个参数hfile.block.cache.size，这个参数在本案例中并不需要设置，没有任何意义。但是HBase的硬规定却是按照这个参数计算的，这个参数的值加上hbase.regionserver.global.memstore.upperLimit的值不能大于0.8，上文提到hbase.regionserver.global.memstore.upperLimit值设置为0.66，因此，hfile.block.cache.size必须设置为一个小于0.14的任意值。hbase.bucketcache.ioengine表示bucketcache设置为offheap模式；hbase.bucketcache.size表示所有读缓存占用内存大小，该值可以为内存真实值，单位为M，也可以为比例值，表示读缓存大小占JVM内存大小比例。如果为内存真实值，则为34G，即34816。hbase.bucketcache.percentage.in.combinedcache参数表示用于缓存用户数据块的内存（堆外内存）占所有读缓存的比例，设为0.90；

---
**HBase RowKey设计思路**

病案详情首页的设计
相关表：
1.病案首页 MedicalRecordHomePage
2.入院记录 ResidentAdmiNote
3.出院记录 DischargeRecord
4.手术记录 OperationRecord
5.首次病程 FirstCourse
6.日常病程 DailyCourse
7.上级查房 Rounds
8.诊断 Diagnose
9.检验医嘱 OrdLis
10.检验项 ListItem
11.检查 OrdRis
12.医嘱 OrderItem

通过登记号+就诊号来获取病例详情病人的相关信息
病人登记号 regNo
病人就诊 admNo
病人信息表 rowkey MD5(regNo)+_+regNo
病人就诊表 rowkey MD5(regNo)+_+regNo+_+admNo
其他表 MD5(regNo)+_+regNo+_+admNo+_+tableName+相关id+_+tableName
000488f7_0007563898_14555617_LR_9511887_LR
startkey = 000488f7_0007563898_14555617
endkey = 000488f7_0007563898_14555618
scan startkey endkey
获取每条result判断rowkey split("_")的tableName 属于哪一类型的Rowkey 存入相关集合

  [1]: http://static.zybuluo.com/chaohan/37t2j2s6wmfi0cugijvv51uk/QQ%E6%88%AA%E5%9B%BE20180622103755.png
  [2]: http://static.zybuluo.com/chaohan/6qrn19s5j0j8h15lythxg2v1/ml2m3ni7xc.png
  [3]: http://static.zybuluo.com/chaohan/g1chetgg8k44u2wnucvkv403/%E5%9B%BE%E7%89%871.jpg
  [4]: http://static.zybuluo.com/chaohan/3j8b6zqfwvu5xkhgbjnp22q0/%E5%9B%BE%E7%89%872.jpg
  [5]: http://static.zybuluo.com/chaohan/012r2lbqxc7tjn7i5mt38cnm/1.jpg
  [6]: http://static.zybuluo.com/chaohan/c92uvs7aq233772othugetu8/2.jpg